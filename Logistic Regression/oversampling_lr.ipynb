{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74b455e-32e5-48d8-a6db-312bec9bceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import holidays\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.ml.classification import RandomForestClassifier, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391abdcf-b60a-4b6c-8eb0-0c4497707508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 12:08:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/15 12:08:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US_Accidents\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cc12b5-bb19-48c1-88df-caf8fd98f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "df = spark.read.parquet(\"final_dataset_revised.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce879d09-6de4-4423-8061-2945112788dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use StringIndexer for encoding the 'Severity' column\n",
    "indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ded403-69b2-4127-8524-206ab62e592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features\n",
    "feature_list = []\n",
    "for col in df.columns:\n",
    "    if col == 'Severity':\n",
    "        continue\n",
    "    elif col == 'SeverityIndex':\n",
    "        continue\n",
    "    else:\n",
    "        feature_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074c3cdd-c11d-4814-bfdd-508d4af82146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3c4f42-35aa-48c1-94f7-4e464e5547a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|Severity|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_15-19|Percent_Age_20-24|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|SeverityIndex|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          1|          3.0|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (87 + 5) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|Severity|  count|\n",
      "+--------+-------+\n",
      "|       1|4524198|\n",
      "|       3|4526752|\n",
      "|       4|4529842|\n",
      "|       2|4527510|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'Severity' and count occurrences\n",
    "class_counts = train.groupBy(\"Severity\").count()\n",
    "\n",
    "# Step 2: Use PySpark's max() function to find the maximum count\n",
    "max_class_size = class_counts.agg(F.max('count')).collect()[0][0]\n",
    "\n",
    "# Initialize a list to store the oversampled DataFrames for each class\n",
    "oversampled_df_list = []\n",
    "\n",
    "# Step 3: Oversample each class to match the maximum class size\n",
    "for row in class_counts.collect():\n",
    "    class_label = row['Severity']\n",
    "    class_size = row['count']\n",
    "\n",
    "    # Filter data for the current class\n",
    "    class_data = train.filter(F.col(\"Severity\") == class_label)\n",
    "\n",
    "    if class_size < max_class_size:\n",
    "        # Oversample the class to match the maximum class size\n",
    "        class_data_oversampled = class_data.sample(withReplacement=True, fraction=max_class_size / class_size)\n",
    "    else:\n",
    "        # Retain the original data for the class if it's already at the maximum size\n",
    "        class_data_oversampled = class_data\n",
    "\n",
    "    # Add the oversampled data to the list\n",
    "    oversampled_df_list.append(class_data_oversampled)\n",
    "\n",
    "# Step 4: Combine all the oversampled DataFrames\n",
    "oversampled_train = oversampled_df_list[0]  # start with the first one\n",
    "for class_df in oversampled_df_list[1:]:\n",
    "    oversampled_train = oversampled_train.union(class_df)\n",
    "\n",
    "# Show the result of oversampling\n",
    "oversampled_train.show()\n",
    "\n",
    "# Step 5: Group by 'Severity' and count the occurrences in the oversampled DataFrame\n",
    "oversampled_class_counts = oversampled_train.groupBy(\"Severity\").count()\n",
    "\n",
    "# Show the result\n",
    "oversampled_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c6cf7b-45cf-4de9-96a9-e1d17a1a2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble data for logistic regression model\n",
    "assembler = VectorAssembler(inputCols=feature_list,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "oversampled_train = assembler.transform(oversampled_train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c0f486-35a1-4b04-95a7-0b1f8b29a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(oversampled_train)\n",
    "scaledTrainData = scalerModel.transform(oversampled_train)\n",
    "scaledTestData = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ae29a-a871-47f3-814b-c19194a38b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 211:======================================================>(91 + 1) / 92]\r"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model with intercept\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='SeverityIndex',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        #maxIter=10, \n",
    "                        #regParam=0.3, \n",
    "                        #elasticNetParam=0.8,\n",
    "                        family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrainData)\n",
    "print(f\"Detected number of classes: {lrModel.numClasses}\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f363636-eb3d-484f-87ad-0e9bc467f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTestData)\n",
    "lrPred.select(\"prediction\").distinct().show()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Severity', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(lrPred, {evaluator.metricName: \"accuracy\"})\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495062f2-b7e4-44b3-be02-080e7feafb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedPrecision'})\n",
    "recall = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedRecall'})\n",
    "f1_score = evaluator.evaluate(lrPred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292cee9-8786-4855-8373-1cfb6bf80255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a column to indicate correct or incorrect predictions\n",
    "predictions = lrPred.withColumn(\n",
    "    'is_correct', F.expr(\"CASE WHEN SeverityIndex = prediction THEN 1 ELSE 0 END\")\n",
    ")\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracy_by_class = predictions.groupBy('SeverityIndex').agg(\n",
    "    (F.sum('is_correct') / F.count('SeverityIndex')).alias('accuracy')\n",
    ")\n",
    "\n",
    "# Show per-class accuracy\n",
    "accuracy_by_class.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
