{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf6ccc1-6317-482a-ac30-72c6eab49a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "#import holidays\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.ml.classification import RandomForestClassifier, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0709727-12cd-4322-b2e8-5ceca8b3b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/26 14:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "24/11/26 14:11:37 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark Session\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"US_Accidents\")\n",
    "  .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a537b4-0516-49a4-a9d3-9957ecf6972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "df = spark.read.parquet(\"updated_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6be913b-8a20-494f-af35-4cc44879acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use StringIndexer for encoding the 'Severity' column\n",
    "indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb96fd0f-7581-467f-a0c3-948425bd9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features\n",
    "feature_list = []\n",
    "for col in df.columns:\n",
    "    if col == 'Severity':\n",
    "        continue\n",
    "    elif col == 'SeverityIndex':\n",
    "        continue\n",
    "    elif col == 'Temperature':\n",
    "        continue\n",
    "    elif col == 'Severity_Binary':\n",
    "        continue\n",
    "    else:\n",
    "        feature_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8117f0e8-de44-4430-aa72-8899adcaeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827fa4c6-689b-4ea8-9b72-5fdbf8d7ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+-------------+\n",
      "|Severity|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|Percent_Age_15-24|Severity_Binary|SeverityIndex|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+-------------+\n",
      "|       2|          1|      60|      30|        10|         7|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     93.5|               25.1|     64790.0|        1.0|                   0|                   1|          0|             10.4|              0|          0.0|\n",
      "|       2|          3|      76|      30|         6|         8|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|     80.3|                9.9|     22593.0|        1.0|                   0|                   0|          0|             20.3|              0|          0.0|\n",
      "|       2|          6|      80|      29|        10|        12|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     94.3|                9.6|     58903.0|        1.0|                   0|                   0|          0|             10.9|              0|          0.0|\n",
      "|       2|          8|      76|      30|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     93.4|               21.7|     88255.0|     0.9429|                   0|                   1|          0|              8.9|              0|          0.0|\n",
      "|       2|         10|      62|      29|        10|         3|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    106.3|                7.1|     32507.0|        1.0|                   0|                   1|          0|             13.4|              0|          0.0|\n",
      "|       2|         10|      67|      29|         5|        16|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    186.7|               20.5|     52578.0|        0.0|                   0|                   0|          1|        6.6000004|              0|          0.0|\n",
      "|       2|         10|      73|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     97.9|               15.3|     80219.0|     0.9996|                   0|                   1|          0|             12.2|              0|          0.0|\n",
      "|       2|         12|      77|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     88.5|               18.9|     83612.0|        1.0|                   0|                   0|          0|             12.7|              0|          0.0|\n",
      "|       2|         14|      59|      30|        10|         8|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     82.2|               13.5|     57014.0|     0.2787|                   0|                   1|          0|             14.0|              0|          0.0|\n",
      "|       2|         15|      29|      30|        10|        16|            0|      0|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     97.7|               13.2|     76970.0|     0.9998|                   0|                   0|          0|             12.6|              0|          0.0|\n",
      "|       2|         15|      56|      29|        10|         5|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     99.0|               14.9|    106861.0|        1.0|                   0|                   1|          0|             18.3|              0|          0.0|\n",
      "|       2|         16|      86|      29|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     95.3|               22.3|     52016.0|        0.0|                   0|                   0|          0|             10.0|              0|          0.0|\n",
      "|       2|         17|      81|      24|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    118.2|               14.1|     52583.0|        0.0|                   0|                   0|          0|        13.700001|              0|          0.0|\n",
      "|       2|         18|      45|      29|        10|        18|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|    121.3|                9.9|     68654.0|        1.0|                   0|                   1|          0|             11.8|              0|          0.0|\n",
      "|       2|         18|      78|      29|        10|         9|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|     93.4|                9.7|     50787.0|        1.0|                   0|                   0|          0|             12.1|              0|          0.0|\n",
      "|       2|         18|      86|      29|         5|        12|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|     97.0|               13.0|    120214.0|     0.6274|                   0|                   1|          0|             11.7|              0|          0.0|\n",
      "|       2|         19|      57|      29|        10|        13|            0|      0|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     96.5|               18.9|     77307.0|     0.6482|                   0|                   0|          0|             10.5|              0|          0.0|\n",
      "|       2|         21|      53|      29|        10|         7|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     99.8|                9.2|     35436.0|        1.0|                   0|                   0|          0|             19.8|              0|          0.0|\n",
      "|       2|         21|      53|      30|        10|         6|            0|      0|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    100.2|               15.1|    109457.0|        1.0|                   0|                   0|          0|        10.700001|              0|          0.0|\n",
      "|       2|         21|      67|      30|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    102.3|               12.2|     85574.0|     0.9991|                   0|                   1|          0|             12.6|              0|          0.0|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================================> (89 + 3) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|SeverityIndex|count|\n",
      "+-------------+-----+\n",
      "|          0.0|52083|\n",
      "|          1.0|52360|\n",
      "|          3.0|52151|\n",
      "|          2.0|52192|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Undersampling\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'Severity' and count occurrences\n",
    "class_counts = train.groupBy(\"SeverityIndex\").count()\n",
    "\n",
    "# Step 2: Use PySpark's min() function to find the minimum count\n",
    "min_class_size = class_counts.agg(F.min('count')).collect()[0][0]\n",
    "\n",
    "undersampled_train_list = []\n",
    "\n",
    "for row in class_counts.collect():\n",
    "    class_label = row['SeverityIndex']\n",
    "    class_size = row['count']\n",
    "\n",
    "    if class_size > min_class_size:\n",
    "        # Sample the data for this class to the size of the minimum class\n",
    "        class_data = train.filter(F.col(\"SeverityIndex\") == class_label)\n",
    "        class_data_undersampled = class_data.sample(withReplacement=False, fraction=min_class_size / class_size)\n",
    "    else:\n",
    "        # For classes that are already at the minimum size, keep all samples\n",
    "        class_data_undersampled = train.filter(F.col(\"SeverityIndex\") == class_label)\n",
    "\n",
    "    undersampled_train_list.append(class_data_undersampled)\n",
    "\n",
    "# Combine all the undersampled DataFrames\n",
    "undersampled_train = undersampled_train_list[0]  # start with the first one\n",
    "for df in undersampled_train_list[1:]:\n",
    "    undersampled_train = undersampled_train.union(df)\n",
    "\n",
    "# Show the result\n",
    "undersampled_train.show()\n",
    "\n",
    "# Step 4: Group by 'Severity' and count the occurrences in the undersampled DataFrame\n",
    "undersampled_class_counts = undersampled_train.groupBy(\"SeverityIndex\").count()\n",
    "\n",
    "# Show the result\n",
    "undersampled_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d79048b-55fa-4cac-9ce6-00d115340220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble data for logistic regression model\n",
    "assembler = VectorAssembler(inputCols=feature_list,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "undersampled_train = assembler.transform(undersampled_train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5e00b4-277f-4dff-b480-5bcb8acdc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(undersampled_train)\n",
    "scaledTrainData = scalerModel.transform(undersampled_train)\n",
    "scaledTestData = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cb60dc-688a-45e9-b2fe-bf00d4ec87f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected number of classes: 4\n",
      "Coefficients: DenseMatrix([[-0.06085354,  0.02512996, -0.03584209, -0.01766107,  0.00533184,\n",
      "              -0.0249659 , -0.04227088, -0.00683355,  0.05261225,  0.53576117,\n",
      "               0.33370228,  0.02736725, -0.00423946, -0.09489827,  0.02276346,\n",
      "               0.02116266,  0.00824079,  0.04667427, -0.00562133, -0.02895272,\n",
      "               0.06028706,  0.00666845],\n",
      "             [-0.03090683,  0.23517944,  0.00671497,  0.12094491,  0.00940698,\n",
      "              -0.06629674, -0.00941883,  0.04681543,  0.06732032,  0.30599878,\n",
      "               0.19289011, -0.06957182, -0.10723529,  0.47167297,  0.05188716,\n",
      "              -0.13250426,  0.01294565,  0.2820368 ,  0.00304459, -0.26898556,\n",
      "              -0.09426524, -0.02519555],\n",
      "             [ 0.02659863, -0.02474515,  0.02764188,  0.0653917 , -0.01807037,\n",
      "              -0.14567814, -0.16736988,  0.01018761,  0.07022371,  0.37094758,\n",
      "               0.20251359,  0.01988612,  0.17561843, -0.00511095,  0.02983813,\n",
      "               0.10058577,  0.00974384, -0.44171503, -0.00252158, -0.01604028,\n",
      "              -0.00657803,  0.01322759],\n",
      "             [ 0.06516174, -0.23556425,  0.00148523, -0.16867555,  0.00333155,\n",
      "               0.23694077,  0.21905959, -0.05016949, -0.19015628, -1.21270753,\n",
      "              -0.72910598,  0.02231845, -0.06414368, -0.37166375, -0.10448875,\n",
      "               0.01075583, -0.03093028,  0.11300396,  0.00509832,  0.31397856,\n",
      "               0.04055621,  0.00529951]])\n",
      "Intercept: [-0.5988203720720675,-6.755057104866027,1.4812979359756775,5.872579540962417]\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model with intercept\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='SeverityIndex',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        #maxIter=10, \n",
    "                        #regParam=0.3, \n",
    "                        #elasticNetParam=0.8,\n",
    "                        family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrainData)\n",
    "print(f\"Detected number of classes: {lrModel.numClasses}\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9646a1f-7a43-4a74-8e3d-e64dd3a11062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       3.0|\n",
      "|       2.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 83:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38584338463552115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTestData)\n",
    "lrPred.select(\"prediction\").distinct().show()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='SeverityIndex', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(lrPred, {evaluator.metricName: \"accuracy\"})\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3e81050-0bf6-4e42-b0c8-f7ebe09b0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:===================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7661970955397004\n",
      "Recall: 0.38584338463552115\n",
      "F1 Score: 0.4775273384563157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "precision = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedPrecision'})\n",
    "recall = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedRecall'})\n",
    "f1_score = evaluator.evaluate(lrPred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e473cd-b658-4a72-ba3a-f5d2752719eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:================================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|SeverityIndex|          accuracy|\n",
      "+-------------+------------------+\n",
      "|          0.0| 0.359863625156528|\n",
      "|          1.0|0.4888015333558131|\n",
      "|          3.0|0.7896235855592333|\n",
      "|          2.0|0.4122885377700854|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a column to indicate correct or incorrect predictions\n",
    "predictions = lrPred.withColumn(\n",
    "    'is_correct', F.expr(\"CASE WHEN SeverityIndex = prediction THEN 1 ELSE 0 END\")\n",
    ")\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracy_by_class = predictions.groupBy('SeverityIndex').agg(\n",
    "    (F.sum('is_correct') / F.count('SeverityIndex')).alias('accuracy')\n",
    ")\n",
    "\n",
    "# Show per-class accuracy\n",
    "accuracy_by_class.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f95c5791-c80a-4c0a-a0a4-abc4c0218cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:===============================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0 - Precision: 0.8852, Recall: 0.3599, F1 Score: 0.5117\n",
      "Class 1.0 - Precision: 0.3231, Recall: 0.4888, F1 Score: 0.3890\n",
      "Class 2.0 - Precision: 0.0625, Recall: 0.4123, F1 Score: 0.1085\n",
      "Class 3.0 - Precision: 0.0279, Recall: 0.7896, F1 Score: 0.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate metrics by class\n",
    "labels = lrPred.select('SeverityIndex').distinct().orderBy('SeverityIndex').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "metrics = {}\n",
    "for label in labels:\n",
    "    # Filter predictions for the current label\n",
    "    true_positive = lrPred.filter((F.col('SeverityIndex') == label) & (F.col('prediction') == label)).count()\n",
    "    false_positive = lrPred.filter((F.col('SeverityIndex') != label) & (F.col('prediction') == label)).count()\n",
    "    false_negative = lrPred.filter((F.col('SeverityIndex') == label) & (F.col('prediction') != label)).count()\n",
    "    true_negative = lrPred.filter((F.col('SeverityIndex') != label) & (F.col('prediction') != label)).count()\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics[label] = {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "\n",
    "# Print metrics for each class\n",
    "for label, metric in metrics.items():\n",
    "    print(f\"Class {label} - Precision: {metric['precision']:.4f}, Recall: {metric['recall']:.4f}, F1 Score: {metric['f1_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
