{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74b455e-32e5-48d8-a6db-312bec9bceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "#import holidays\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.ml.classification import RandomForestClassifier, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391abdcf-b60a-4b6c-8eb0-0c4497707508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 14:35:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/25 14:35:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/25 14:35:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/25 14:35:09 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/25 14:35:09 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US_Accidents\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #supress warnings\n",
    "#spark.conf.set(\"spark.python.worker.reuse\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cc12b5-bb19-48c1-88df-caf8fd98f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read in Data\n",
    "df = spark.read.parquet(\"updated_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce879d09-6de4-4423-8061-2945112788dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use StringIndexer for encoding the 'Severity' column\n",
    "indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ded403-69b2-4127-8524-206ab62e592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features\n",
    "feature_list = []\n",
    "for col in df.columns:\n",
    "    if col == 'Severity':\n",
    "        continue\n",
    "    elif col == 'SeverityIndex':\n",
    "        continue\n",
    "    elif col == 'Severity_Binary':\n",
    "        continue\n",
    "    else:\n",
    "        feature_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074c3cdd-c11d-4814-bfdd-508d4af82146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3c4f42-35aa-48c1-94f7-4e464e5547a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+-------------+\n",
      "|Severity|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|Percent_Age_15-24|Severity_Binary|SeverityIndex|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+-------------+\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "|       1|          9|      61|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|               14.6|     48055.0|        1.0|                   0|                   1|          1|             11.1|              0|          3.0|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================================> (89 + 3) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|Severity|  count|\n",
      "+--------+-------+\n",
      "|       1|4523565|\n",
      "|       3|4529901|\n",
      "|       4|4525628|\n",
      "|       2|4527457|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'Severity' and count occurrences\n",
    "class_counts = train.groupBy(\"Severity\").count()\n",
    "\n",
    "# Step 2: Use PySpark's max() function to find the maximum count\n",
    "max_class_size = class_counts.agg(F.max('count')).collect()[0][0]\n",
    "\n",
    "# Initialize a list to store the oversampled DataFrames for each class\n",
    "oversampled_df_list = []\n",
    "\n",
    "# Step 3: Oversample each class to match the maximum class size\n",
    "for row in class_counts.collect():\n",
    "    class_label = row['Severity']\n",
    "    class_size = row['count']\n",
    "\n",
    "    # Filter data for the current class\n",
    "    class_data = train.filter(F.col(\"Severity\") == class_label)\n",
    "\n",
    "    if class_size < max_class_size:\n",
    "        # Oversample the class to match the maximum class size\n",
    "        class_data_oversampled = class_data.sample(withReplacement=True, fraction=max_class_size / class_size)\n",
    "    else:\n",
    "        # Retain the original data for the class if it's already at the maximum size\n",
    "        class_data_oversampled = class_data\n",
    "\n",
    "    # Add the oversampled data to the list\n",
    "    oversampled_df_list.append(class_data_oversampled)\n",
    "\n",
    "# Step 4: Combine all the oversampled DataFrames\n",
    "oversampled_train = oversampled_df_list[0]  # start with the first one\n",
    "for class_df in oversampled_df_list[1:]:\n",
    "    oversampled_train = oversampled_train.union(class_df)\n",
    "\n",
    "# Show the result of oversampling\n",
    "oversampled_train.show()\n",
    "\n",
    "# Step 5: Group by 'Severity' and count the occurrences in the oversampled DataFrame\n",
    "oversampled_class_counts = oversampled_train.groupBy(\"Severity\").count()\n",
    "\n",
    "# Show the result\n",
    "oversampled_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c6cf7b-45cf-4de9-96a9-e1d17a1a2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble data for logistic regression model\n",
    "assembler = VectorAssembler(inputCols=feature_list,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "oversampled_train = assembler.transform(oversampled_train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c0f486-35a1-4b04-95a7-0b1f8b29a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(oversampled_train)\n",
    "scaledTrainData = scalerModel.transform(oversampled_train)\n",
    "scaledTestData = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48ae29a-a871-47f3-814b-c19194a38b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected number of classes: 4\n",
      "Coefficients: DenseMatrix([[-1.45363390e-02, -7.58303790e-02,  2.91937057e-02,\n",
      "              -3.74094275e-02, -1.24750825e-02,  6.96323113e-04,\n",
      "              -3.32754450e-02, -4.13082899e-02, -1.03104336e-02,\n",
      "               3.92842259e-02,  5.31494321e-01,  3.34042359e-01,\n",
      "               2.01782267e-02, -1.65233702e-03, -1.00455642e-01,\n",
      "               2.69371564e-02,  2.72164880e-02,  8.13547771e-03,\n",
      "               3.88685492e-02, -1.11537199e-02, -2.82183257e-02,\n",
      "               6.03423892e-02,  3.42269404e-03],\n",
      "             [-1.47098705e-01, -7.93169750e-02,  2.34377584e-01,\n",
      "               2.32671640e-03,  1.29782872e-01,  7.22770894e-03,\n",
      "              -6.37089246e-02, -1.62654732e-02,  4.61424749e-02,\n",
      "               3.38121905e-02,  2.21116128e-01,  1.55262087e-01,\n",
      "              -1.20222472e-01, -1.21768660e-01,  4.69349218e-01,\n",
      "               5.58349069e-02, -1.26020508e-01,  1.20803937e-03,\n",
      "               2.88624360e-01,  5.25601320e-03, -2.63215556e-01,\n",
      "              -9.92309562e-02, -2.84164229e-02],\n",
      "             [-2.44660473e-01, -4.62399024e-02,  1.39694793e-02,\n",
      "               2.90026778e-02,  6.86958776e-02, -7.26420664e-03,\n",
      "              -1.38722126e-01, -1.69595761e-01,  1.46485352e-02,\n",
      "               3.53192598e-02,  2.18211943e-01,  1.43328185e-01,\n",
      "              -7.13556640e-02,  1.64652111e-01, -7.46490937e-03,\n",
      "               4.57265127e-03,  9.45050285e-02, -2.10581783e-02,\n",
      "              -4.31892036e-01, -4.26135074e-03, -1.85308179e-02,\n",
      "              -8.66401663e-03,  1.22717814e-02],\n",
      "             [ 4.06295516e-01,  2.01387256e-01, -2.77540769e-01,\n",
      "               6.08003336e-03, -1.86003667e-01, -6.59825414e-04,\n",
      "               2.35706496e-01,  2.27169524e-01, -5.04805765e-02,\n",
      "              -1.08415676e-01, -9.70822391e-01, -6.32632631e-01,\n",
      "               1.71399909e-01, -4.12311141e-02, -3.61428667e-01,\n",
      "              -8.73447145e-02,  4.29899158e-03,  1.17146612e-02,\n",
      "               1.04399127e-01,  1.01590574e-02,  3.09964700e-01,\n",
      "               4.75525837e-02,  1.27219475e-02]])\n",
      "Intercept: [-0.5767419771026508,-6.009051057772729,1.7929275921478536,4.792865442727527]\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model with intercept\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='SeverityIndex',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrainData)\n",
    "print(f\"Detected number of classes: {lrModel.numClasses}\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f363636-eb3d-484f-87ad-0e9bc467f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       3.0|\n",
      "|       2.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:===========================================>            (18 + 5) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17727210646270988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTestData)\n",
    "lrPred.select(\"prediction\").distinct().show()\n",
    "\n",
    "# Assuming `lrPred` is your predictions DataFrame\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity_Binary\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName='truePositiveRateByLabel', \n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "recall = evaluator.evaluate(lrPred)\n",
    "print(f\"Recall for label 1.0: {recall}\")\n",
    "\n",
    "accuracy = evaluator.evaluate(lrPred, {evaluator.metricName: \"accuracy\"})\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495062f2-b7e4-44b3-be02-080e7feafb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:=====================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6926750188675712\n",
      "Recall: 0.17727210646270986\n",
      "F1 Score: 0.2627434249030147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "precision = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedPrecision'})\n",
    "recall = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedRecall'})\n",
    "f1_score = evaluator.evaluate(lrPred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e292cee9-8786-4855-8373-1cfb6bf80255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 99:==============================================>         (19 + 4) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|SeverityIndex|           accuracy|\n",
      "+-------------+-------------------+\n",
      "|          0.0| 0.3596577196450649|\n",
      "|          1.0| 0.5000532415212877|\n",
      "|          3.0| 0.7769224847971673|\n",
      "|          2.0|0.42437608173747976|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a column to indicate correct or incorrect predictions\n",
    "predictions = lrPred.withColumn(\n",
    "    'is_correct', F.expr(\"CASE WHEN SeverityIndex = prediction THEN 1 ELSE 0 END\")\n",
    ")\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracy_by_class = predictions.groupBy('SeverityIndex').agg(\n",
    "    (F.sum('is_correct') / F.count('SeverityIndex')).alias('accuracy')\n",
    ")\n",
    "\n",
    "# Show per-class accuracy\n",
    "accuracy_by_class.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24ea1768-178d-4dc8-9371-83413f6290cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:==================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|SeverityIndex|\n",
      "+-------------+\n",
      "|          0.0|\n",
      "|          1.0|\n",
      "|          3.0|\n",
      "|          2.0|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distinct_labels = lrPred.select('SeverityIndex').distinct()\n",
    "distinct_labels.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c369e6a-ee0e-4f30-a5e9-49fc4d447abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 158:==================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0 - Precision: 0.8876, Recall: 0.3597, F1 Score: 0.5119\n",
      "Class 1.0 - Precision: 0.3225, Recall: 0.5001, F1 Score: 0.3921\n",
      "Class 2.0 - Precision: 0.0625, Recall: 0.4244, F1 Score: 0.1089\n",
      "Class 3.0 - Precision: 0.0285, Recall: 0.7769, F1 Score: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate metrics by class\n",
    "labels = [row['SeverityIndex'] for row in lrPred.select('SeverityIndex').distinct().orderBy('SeverityIndex').collect()]\n",
    "\n",
    "metrics = {}\n",
    "for label in labels:\n",
    "    # Filter predictions for the current label\n",
    "    true_positive = lrPred.filter((F.col('SeverityIndex') == label) & (F.col('prediction') == label)).count()\n",
    "    false_positive = lrPred.filter((F.col('SeverityIndex') != label) & (F.col('prediction') == label)).count()\n",
    "    false_negative = lrPred.filter((F.col('SeverityIndex') == label) & (F.col('prediction') != label)).count()\n",
    "    true_negative = lrPred.filter((F.col('SeverityIndex') != label) & (F.col('prediction') != label)).count()\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics[label] = {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "\n",
    "# Print metrics for each class\n",
    "for label, metric in metrics.items():\n",
    "    print(f\"Class {label} - Precision: {metric['precision']:.4f}, Recall: {metric['recall']:.4f}, F1 Score: {metric['f1_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
