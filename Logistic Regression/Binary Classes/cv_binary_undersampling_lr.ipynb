{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf6ccc1-6317-482a-ac30-72c6eab49a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "#import holidays\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.ml.classification import RandomForestClassifier, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0709727-12cd-4322-b2e8-5ceca8b3b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/27 17:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US_Accidents\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a537b4-0516-49a4-a9d3-9957ecf6972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "df = spark.read.parquet(\"updated_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c544971f-8cf8-4057-9dbf-0e409200cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+\n",
      "|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|Percent_Age_15-24|Severity_Binary|\n",
      "+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+\n",
      "|         21|      85|      30|         1|        10|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|     97.6|               13.1|    104583.0|        1.0|                   0|                   1|          0|             12.0|              0|\n",
      "|         51|      29|      29|        10|         8|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|     89.6|               12.7|    128378.0|      0.982|                   0|                   0|          0|             13.5|              0|\n",
      "|         55|      83|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     94.6|               17.3|     52577.0|     0.9968|                   0|                   0|          0|             12.3|              1|\n",
      "|         52|      94|      30|        10|         9|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    106.8|               11.6|     48740.0|        1.0|                   0|                   0|          0|             14.6|              0|\n",
      "|         25|      96|      30|         0|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     95.9|               14.4|     62956.0|     0.9887|                   0|                   0|          0|             12.4|              0|\n",
      "+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification (1 or 2 vs 3 or 4)\n",
    "#df = df.withColumn('Severity_Binary', when((col(\"Severity\")==1) | (col(\"Severity\")==2), 0).otherwise(1))\n",
    "df = df.drop('Severity')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb96fd0f-7581-467f-a0c3-948425bd9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features\n",
    "feature_list = []\n",
    "for col in df.columns:\n",
    "    if col == 'Severity':\n",
    "        continue\n",
    "    elif col == 'Severity_Binary':\n",
    "        continue\n",
    "    else:\n",
    "        feature_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8117f0e8-de44-4430-aa72-8899adcaeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827fa4c6-689b-4ea8-9b72-5fdbf8d7ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+\n",
      "|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|Percent_Age_15-24|Severity_Binary|\n",
      "+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+\n",
      "|        -17|      80|      29|        10|         6|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   1|     99.1|               17.0|     73628.0|        0.0|                   0|                   0|          0|             12.7|              1|\n",
      "|        -17|      81|      29|        10|         3|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|     97.2|               19.3|     61710.0|     0.5932|                   0|                   0|          0|             11.2|              1|\n",
      "|        -17|      81|      29|        10|         3|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|    103.6|               18.5|     60634.0|        0.0|                   0|                   0|          0|             12.0|              1|\n",
      "|        -16|      79|      29|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    112.1|                7.3|     59917.0|        1.0|                   0|                   0|          0|             17.4|              1|\n",
      "|        -16|      79|      29|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    141.0|                3.4|    103750.0|        1.0|                   0|                   1|          0|              8.4|              1|\n",
      "|        -16|      79|      30|         2|        13|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     91.0|                9.6|     63618.0|        1.0|                   0|                   0|          0|             19.7|              1|\n",
      "|        -15|      75|      29|        10|         6|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|    112.6|               10.2|     58939.0|        1.0|                   0|                   0|          0|              9.2|              1|\n",
      "|        -15|      84|      29|        10|         8|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.8|               12.1|     93372.0|     0.7635|                   0|                   1|          0|        11.700001|              1|\n",
      "|        -13|      71|      29|        10|         9|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.5|                7.1|     41110.0|        1.0|                   0|                   0|          0|             16.6|              1|\n",
      "|        -13|      75|      29|        10|         8|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|    103.9|                9.9|     52199.0|     0.8638|                   0|                   0|          0|             30.2|              1|\n",
      "|        -12|      78|      29|        10|         5|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|    103.9|               17.2|     78705.0|        0.0|                   0|                   0|          0|             11.3|              1|\n",
      "|        -12|      78|      29|        10|         6|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   1|    103.9|                9.9|     52199.0|     0.8638|                   0|                   0|          0|             30.2|              1|\n",
      "|        -12|      78|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|    107.9|               15.7|     49412.0|     0.9924|                   0|                   0|          0|             14.7|              1|\n",
      "|        -11|      75|      29|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|    129.3|               11.6|     48682.0|        1.0|                   0|                   0|          0|        11.900001|              1|\n",
      "|        -10|      74|      29|        10|        10|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    112.6|               10.2|     58939.0|        1.0|                   0|                   0|          0|              9.2|              1|\n",
      "|        -10|      86|      29|        10|         0|            0|      0|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     90.0|               10.9|    127570.0|     0.9978|                   0|                   0|          0|             11.0|              1|\n",
      "|         -9|      73|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     82.4|               13.1|     44444.0|        1.0|                   0|                   0|          0|        15.700001|              1|\n",
      "|         -9|      73|      30|        10|         6|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     98.3|               12.4|     47996.0|        1.0|                   0|                   1|          0|             13.4|              1|\n",
      "|         -9|      74|      30|        10|         5|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|    100.7|                7.4|     40885.0|        1.0|                   0|                   0|          0|             14.8|              1|\n",
      "|         -9|      75|      30|        10|         0|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|     98.9|               10.9|    120613.0|        1.0|                   0|                   0|          0|             12.7|              1|\n",
      "+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-------------------+------------+-----------+--------------------+--------------------+-----------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================================> (45 + 1) / 46]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Severity_Binary|  count|\n",
      "+---------------+-------+\n",
      "|              1|1041734|\n",
      "|              0|1041973|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Undersampling\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'Severity' and count occurrences\n",
    "class_counts = train.groupBy(\"Severity_Binary\").count()\n",
    "\n",
    "# Step 2: Use PySpark's min() function to find the minimum count\n",
    "min_class_size = class_counts.agg(F.min('count')).collect()[0][0]\n",
    "\n",
    "undersampled_train_list = []\n",
    "\n",
    "for row in class_counts.collect():\n",
    "    class_label = row['Severity_Binary']\n",
    "    class_size = row['count']\n",
    "\n",
    "    if class_size > min_class_size:\n",
    "        # Sample the data for this class to the size of the minimum class\n",
    "        class_data = train.filter(F.col(\"Severity_Binary\") == class_label)\n",
    "        class_data_undersampled = class_data.sample(withReplacement=False, fraction=min_class_size / class_size)\n",
    "    else:\n",
    "        # For classes that are already at the minimum size, keep all samples\n",
    "        class_data_undersampled = train.filter(F.col(\"Severity_Binary\") == class_label)\n",
    "\n",
    "    undersampled_train_list.append(class_data_undersampled)\n",
    "\n",
    "# Combine all the undersampled DataFrames\n",
    "undersampled_train = undersampled_train_list[0]  # start with the first one\n",
    "for df in undersampled_train_list[1:]:\n",
    "    undersampled_train = undersampled_train.union(df)\n",
    "\n",
    "# Show the result\n",
    "undersampled_train.show()\n",
    "\n",
    "# Step 4: Group by 'Severity' and count the occurrences in the undersampled DataFrame\n",
    "undersampled_class_counts = undersampled_train.groupBy(\"Severity_Binary\").count()\n",
    "\n",
    "# Show the result\n",
    "undersampled_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d79048b-55fa-4cac-9ce6-00d115340220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble data for logistic regression model\n",
    "assembler = VectorAssembler(inputCols=feature_list,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "undersampled_train = assembler.transform(undersampled_train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5e00b4-277f-4dff-b480-5bcb8acdc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(undersampled_train)\n",
    "scaledTrainData = scalerModel.transform(undersampled_train)\n",
    "scaledTestData = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd234c4-256e-49fa-89b2-759ec1390e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: maxIter=50, regParam=0.0, elasticNetParam=0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"10-Fold Cross Validation Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the model and set the label column\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"Severity_Binary\", family=\"multinomial\")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [50, 100, 150]) \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Severity_Binary\", metricName=\"accuracy\")\n",
    "\n",
    "# Define the CrossValidator\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=10,  #for 10-fold cross-validation\n",
    "    parallelism=4,  #number of threads for parallelism\n",
    "    seed = 314\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_model = cross_validator.fit(scaledTrainData)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best Parameters: maxIter={best_model._java_obj.getMaxIter()}, \"\n",
    "      f\"regParam={best_model._java_obj.getRegParam()}, \"\n",
    "      f\"elasticNetParam={best_model._java_obj.getElasticNetParam()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54cb60dc-688a-45e9-b2fe-bf00d4ec87f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected number of classes: 2\n",
      "Coefficients: DenseMatrix([[-0.16171334,  0.01152171,  0.15186361,  0.043939  ,  0.14509557,\n",
      "               0.00772662, -0.0529401 , -0.00059221,  0.05313852, -0.01431842,\n",
      "              -0.33223754, -0.1867478 , -0.11486773, -0.04695482,  0.55721657,\n",
      "               0.03013459, -0.10489845, -0.00954275,  0.04562231,  0.01501473,\n",
      "              -0.19429149, -0.13248229, -0.03185246]])\n",
      "Intercept: [-3.9061373251514206]\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model with intercept\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='Severity_Binary',\n",
    "                        featuresCol='scaledFeatures', \n",
    "                        maxIter=50, \n",
    "                        regParam=0.0, \n",
    "                        elasticNetParam=0.0)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrainData)\n",
    "print(f\"Detected number of classes: {lrModel.numClasses}\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9646a1f-7a43-4a74-8e3d-e64dd3a11062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for label 1.0: 0.572740584009874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6593:==============================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6870189530711601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTestData)\n",
    "lrPred.select(\"prediction\").distinct().show()\n",
    "\n",
    "# Assuming `lrPred` is your predictions DataFrame\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Severity_Binary\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName='truePositiveRateByLabel', \n",
    "    metricLabel=1.0\n",
    ")\n",
    "\n",
    "recall = evaluator.evaluate(lrPred)\n",
    "print(f\"Recall for label 1.0: {recall}\")\n",
    "\n",
    "accuracy = evaluator.evaluate(lrPred, {evaluator.metricName: \"accuracy\"})\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e81050-0bf6-4e42-b0c8-f7ebe09b0971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6597:==============================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7745993338640202\n",
      "F1 Score: 0.7166056755375578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "precision = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedPrecision'})\n",
    "#recall = evaluator.evaluate(lrPred, {evaluator.metricName: 'truePositiveRateByLabel'})\n",
    "f1_score = evaluator.evaluate(lrPred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "#print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e473cd-b658-4a72-ba3a-f5d2752719eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6599:==============================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|Severity_Binary|          accuracy|\n",
      "+---------------+------------------+\n",
      "|              1| 0.572740584009874|\n",
      "|              0|0.7130592836766815|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a column to indicate correct or incorrect predictions\n",
    "predictions = lrPred.withColumn(\n",
    "    'is_correct', F.expr(\"CASE WHEN Severity_Binary = prediction THEN 1 ELSE 0 END\")\n",
    ")\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracy_by_class = predictions.groupBy('Severity_Binary').agg(\n",
    "    (F.sum('is_correct') / F.count('Severity_Binary')).alias('accuracy')\n",
    ")\n",
    "\n",
    "# Show per-class accuracy\n",
    "accuracy_by_class.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3e6396-e30a-4f8a-a489-9f4be649f78e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:================================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Precision: 0.8801, Recall: 0.7112, F1 Score: 0.7866\n",
      "Class 1 - Precision: 0.3119, Recall: 0.5747, F1 Score: 0.4044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate metrics by class\n",
    "labels = [row['Severity_Binary'] for row in lrPred.select('Severity_Binary').distinct().orderBy('Severity_Binary').collect()]\n",
    "\n",
    "metrics = {}\n",
    "for label in labels:\n",
    "    # Filter predictions for the current label\n",
    "    true_positive = lrPred.filter((F.col('Severity_Binary') == label) & (F.col('prediction') == label)).count()\n",
    "    false_positive = lrPred.filter((F.col('Severity_Binary') != label) & (F.col('prediction') == label)).count()\n",
    "    false_negative = lrPred.filter((F.col('Severity_Binary') == label) & (F.col('prediction') != label)).count()\n",
    "    true_negative = lrPred.filter((F.col('Severity_Binary') != label) & (F.col('prediction') != label)).count()\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # Store metrics\n",
    "    metrics[label] = {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "\n",
    "# Print metrics for each class\n",
    "for label, metric in metrics.items():\n",
    "    print(f\"Class {label} - Precision: {metric['precision']:.4f}, Recall: {metric['recall']:.4f}, F1 Score: {metric['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c042e27-104d-47d4-95f9-a48181d0620e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "Actual \\ Predicted\n",
      "\t0\t1\n",
      "0\t816383\t328519\n",
      "1\t111466\t149420\n"
     ]
    }
   ],
   "source": [
    "# Calculate unique labels\n",
    "labels = [row['Severity_Binary'] for row in lrPred.select('Severity_Binary').distinct().orderBy('Severity_Binary').collect()]\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = {}\n",
    "\n",
    "confusion_matrix = {label: {l: 0 for l in labels} for label in labels}\n",
    "\n",
    "# Update the confusion matrix\n",
    "confusion_matrix_df = (\n",
    "    lrPred\n",
    "    .groupBy(\"Severity_Binary\", \"prediction\")\n",
    "    .count()\n",
    "    .rdd\n",
    "    .map(lambda row: Row(\n",
    "        actual=row[\"Severity_Binary\"], \n",
    "        predicted=row[\"prediction\"], \n",
    "        count=row[\"count\"]\n",
    "    ))\n",
    "    .toDF()\n",
    ")\n",
    "\n",
    "for row in confusion_matrix_df.collect():\n",
    "    confusion_matrix[row['actual']][row['predicted']] = row['count']\n",
    "\n",
    "# Print metrics for each class\n",
    "for label, metric in metrics.items():\n",
    "    print(f\"Class {label} - Precision: {metric['precision']:.4f}, Recall: {metric['recall']:.4f}, F1 Score: {metric['f1_score']:.4f}\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"Actual \\\\ Predicted\")\n",
    "header = \"\\t\" + \"\\t\".join([str(l) for l in labels])\n",
    "print(header)\n",
    "\n",
    "for actual, predictions in confusion_matrix.items():\n",
    "    row = [str(predictions[p]) for p in labels]\n",
    "    print(f\"{actual}\\t\" + \"\\t\".join(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
