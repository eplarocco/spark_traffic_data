{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf6ccc1-6317-482a-ac30-72c6eab49a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import holidays\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.ml.classification import RandomForestClassifier, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0709727-12cd-4322-b2e8-5ceca8b3b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/14 19:43:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "24/11/14 19:43:57 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Spark Session\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"US_Accidents\")\n",
    "  .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a537b4-0516-49a4-a9d3-9957ecf6972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "df = spark.read.parquet(\"final_dataset_revised.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6be913b-8a20-494f-af35-4cc44879acb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use StringIndexer for encoding the 'Severity' column\n",
    "indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb96fd0f-7581-467f-a0c3-948425bd9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of features\n",
    "feature_list = []\n",
    "for col in df.columns:\n",
    "    if col == 'Severity':\n",
    "        continue\n",
    "    elif col == 'SeverityIndex':\n",
    "        continue\n",
    "    elif col == 'SeasonVec':\n",
    "        continue\n",
    "    else:\n",
    "        feature_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8117f0e8-de44-4430-aa72-8899adcaeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827fa4c6-689b-4ea8-9b72-5fdbf8d7ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|Severity|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_15-19|Percent_Age_20-24|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|SeverityIndex|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|       2|         -2|      79|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     81.0|             13.8|             11.0|               17.7|    121037.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|          4|      76|      30|        10|         5|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     95.9|              4.3|              4.5|               18.5|     75011.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|          7|      64|      30|        10|        13|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     93.5|              5.3|              5.1|               25.1|     64790.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|          8|      87|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     93.7|              7.9|              3.6|               18.9|    195494.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         10|      73|      30|        10|         9|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|     88.0|              4.7|             10.8|               13.6|     61577.0|     0.8573|                   0|                   1|          0|          0.0|\n",
      "|       2|         11|      51|      30|        10|        13|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    101.3|              7.9|              9.1|                8.4|     45114.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|         11|      85|      29|         1|         7|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|    104.8|              5.0|              4.8|               12.6|     92237.0|        0.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         12|      59|      29|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|    121.3|              6.5|              5.3|                9.9|     68654.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         14|      88|      30|         1|        12|            0|      1|        0|   0|   1|(3,[0],[1.0])|                         1|                   0|     93.4|              5.0|              3.9|               21.7|     88255.0|     0.9429|                   0|                   0|          0|          0.0|\n",
      "|       2|         15|      70|      29|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     78.4|              7.3|              6.3|               18.4|     27374.0|     0.9091|                   0|                   1|          0|          0.0|\n",
      "|       2|         15|      74|      29|         7|        12|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         1|                   0|     93.9|              7.8|              5.9|               14.0|     79765.0|     0.4178|                   0|                   0|          0|          0.0|\n",
      "|       2|         15|      80|      29|         9|         8|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     97.3|              5.8|              5.8|               18.3|     50902.0|      0.819|                   0|                   0|          0|          0.0|\n",
      "|       2|         15|      84|      29|         2|        29|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|     93.0|              5.2|              5.1|               19.9|     92408.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|         16|      74|      29|        10|         5|            0|      0|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     98.4|              6.2|              3.8|               11.7|    109742.0|     0.9985|                   0|                   0|          0|          0.0|\n",
      "|       2|         16|      86|      29|         3|        15|            0|      1|        0|   0|   1|(3,[0],[1.0])|                         1|                   1|     97.9|              5.5|              7.8|               15.0|     85721.0|     0.6978|                   0|                   1|          0|          0.0|\n",
      "|       2|         17|      56|      30|        10|         8|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|    102.5|              9.6|              2.9|               14.7|    196964.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         18|      88|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     96.4|              6.3|              7.1|               15.1|     49911.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         19|      33|      30|        10|         3|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     99.6|              3.9|              8.3|               13.8|     53525.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         19|      50|      30|        10|        13|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.1|              8.3|              9.4|                8.1|     30726.0|        1.0|                   1|                   1|          0|          0.0|\n",
      "|       2|         19|      71|      30|        10|         9|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    103.9|              4.0|              7.1|               14.6|     48055.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>     (83 + 9) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|SeverityIndex|count|\n",
      "+-------------+-----+\n",
      "|          0.0|51612|\n",
      "|          1.0|52145|\n",
      "|          3.0|52136|\n",
      "|          2.0|52236|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Undersampling\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'Severity' and count occurrences\n",
    "class_counts = train.groupBy(\"SeverityIndex\").count()\n",
    "\n",
    "# Step 2: Use PySpark's min() function to find the minimum count\n",
    "min_class_size = class_counts.agg(F.min('count')).collect()[0][0]\n",
    "\n",
    "undersampled_train_list = []\n",
    "\n",
    "for row in class_counts.collect():\n",
    "    class_label = row['SeverityIndex']\n",
    "    class_size = row['count']\n",
    "\n",
    "    if class_size > min_class_size:\n",
    "        # Sample the data for this class to the size of the minimum class\n",
    "        class_data = train.filter(F.col(\"SeverityIndex\") == class_label)\n",
    "        class_data_undersampled = class_data.sample(withReplacement=False, fraction=min_class_size / class_size)\n",
    "    else:\n",
    "        # For classes that are already at the minimum size, keep all samples\n",
    "        class_data_undersampled = train.filter(F.col(\"SeverityIndex\") == class_label)\n",
    "\n",
    "    undersampled_train_list.append(class_data_undersampled)\n",
    "\n",
    "# Combine all the undersampled DataFrames\n",
    "undersampled_train = undersampled_train_list[0]  # start with the first one\n",
    "for df in undersampled_train_list[1:]:\n",
    "    undersampled_train = undersampled_train.union(df)\n",
    "\n",
    "# Show the result\n",
    "undersampled_train.show()\n",
    "\n",
    "# Step 4: Group by 'Severity' and count the occurrences in the undersampled DataFrame\n",
    "undersampled_class_counts = undersampled_train.groupBy(\"SeverityIndex\").count()\n",
    "\n",
    "# Show the result\n",
    "undersampled_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d79048b-55fa-4cac-9ce6-00d115340220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble data for logistic regression model\n",
    "assembler = VectorAssembler(inputCols=feature_list,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "undersampled_train = assembler.transform(undersampled_train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5e00b4-277f-4dff-b480-5bcb8acdc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(undersampled_train)\n",
    "scaledTrainData = scalerModel.transform(undersampled_train)\n",
    "scaledTestData = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb60dc-688a-45e9-b2fe-bf00d4ec87f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model with intercept\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='SeverityIndex',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        #maxIter=10, \n",
    "                        #regParam=0.3, \n",
    "                        #elasticNetParam=0.8,\n",
    "                        family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrainData)\n",
    "print(f\"Detected number of classes: {lrModel.numClasses}\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9646a1f-7a43-4a74-8e3d-e64dd3a11062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTestData)\n",
    "lrPred.select(\"prediction\").distinct().show()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='SeverityIndex', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(lrPred, {evaluator.metricName: \"accuracy\"})\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e81050-0bf6-4e42-b0c8-f7ebe09b0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedPrecision'})\n",
    "recall = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedRecall'})\n",
    "f1_score = evaluator.evaluate(lrPred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e473cd-b658-4a72-ba3a-f5d2752719eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to indicate correct or incorrect predictions\n",
    "predictions = lrPred.withColumn(\n",
    "    'is_correct', F.expr(\"CASE WHEN SeverityIndex = prediction THEN 1 ELSE 0 END\")\n",
    ")\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracy_by_class = predictions.groupBy('SeverityIndex').agg(\n",
    "    (F.sum('is_correct') / F.count('SeverityIndex')).alias('accuracy')\n",
    ")\n",
    "\n",
    "# Show per-class accuracy\n",
    "accuracy_by_class.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce0c2c-f792-47cd-9e69-bdbbe886c5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
