{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question:\n",
    "### What are the most influential variables on the severity of accidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Paper:\n",
    "    https://www.sciencedirect.com/science/article/pii/S2590198223000611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Display Spark Output in scrollable format within jupyter notebook\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PySpark imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Additional libraries for visualization and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Boost-specific modules for PySpark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/02 18:21:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('GBoost-Traffic-Accidents') \\\n",
    "    .config(\"spark.executor.memory\", \"24g\")\\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.num.executors\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+\n",
      "|Severity|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_15-19|Percent_Age_20-24|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+\n",
      "|       2|         21|      85|      30|         1|        10|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|     97.6|              8.0|              4.0|               13.1|    104583.0|        1.0|                   0|                   1|          0|\n",
      "|       2|         51|      29|      29|        10|         8|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         1|                   0|     89.6|              7.7|              5.8|               12.7|    128378.0|      0.982|                   0|                   0|          0|\n",
      "|       3|         55|      83|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     94.6|              6.9|              5.4|               17.3|     52577.0|     0.9968|                   0|                   0|          0|\n",
      "|       2|         52|      94|      30|        10|         9|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    106.8|              6.4|              8.2|               11.6|     48740.0|        1.0|                   0|                   0|          0|\n",
      "|       2|         25|      96|      30|         0|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     95.9|              5.6|              6.8|               14.4|     62956.0|     0.9887|                   0|                   0|          0|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in Dataset\n",
    "df = spark.read.parquet(\"final_dataset_revised.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Rows count : 7026806\n",
      "DataFrame Columns count : 23\n"
     ]
    }
   ],
   "source": [
    "# Get row count\n",
    "rows = df.count()\n",
    "print(f\"DataFrame Rows count : {rows}\")\n",
    "\n",
    "# Get columns count\n",
    "cols = len(df.columns)\n",
    "print(f\"DataFrame Columns count : {cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------------+\n",
      "|Severity|  count|           percent|\n",
      "+--------+-------+------------------+\n",
      "|       1|  65142|0.9270499285165977|\n",
      "|       3|1123799|15.993027272988611|\n",
      "|       4| 178821|2.5448404296347444|\n",
      "|       2|5659044| 80.53508236886005|\n",
      "+--------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check Class Imbalance\n",
    "import pyspark.sql.functions as F\n",
    "cts = df.groupBy(\"Severity\").count().withColumn('percent', (F.col('count') / rows)*100)\n",
    "cts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_weights = df.withColumn(\n",
    "    \"Severity_Binary\", \n",
    "    F.when(df[\"Severity\"] >= 3, 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {1: 5.39436366707098, 0: 1.2275642335870987}\n"
     ]
    }
   ],
   "source": [
    "class_counts = df_with_weights.groupBy('Severity_Binary').count().collect()\n",
    "total_count = df_with_weights.count()\n",
    "\n",
    "# Ensure that we have counts for both classes\n",
    "class_weights = {}\n",
    "for row in class_counts:\n",
    "    class_weights[row['Severity_Binary']] = total_count / row['count']\n",
    "\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_weights = df_with_weights.withColumn(\n",
    "    'weight', \n",
    "    F.when(df_with_weights['Severity_Binary'] == 1, class_weights[1])\n",
    "     .when(df_with_weights['Severity_Binary'] == 0, class_weights[0])\n",
    ")\n",
    "\n",
    "# Step 3: Drop rows with missing values in 'Severity_Binary' and 'weight' columns\n",
    "df_with_weights = df_with_weights.dropna(subset=['Severity_Binary', 'weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_cols = ['Temperature', 'Humidity', 'Pressure', 'Visibility', 'Wind_Speed',\n",
    "                'Precipitation', 'Weekday', 'Rush_Hour', 'Rain', 'Snow', 'SeasonVec',\n",
    "                'Astronomical_TwilightIndex', 'Interstate_Indicator', 'Sex_ratio',\n",
    "                'Percent_Age_15-19', 'Percent_Age_20-24', 'Percent_Age_65_over', 'MedianIncome',\n",
    "                'Urban_Ratio', 'Traffic_Interference', 'Traffic_Intersection', 'Destination']\n",
    "\n",
    "# Assemble the feature columns into a single vector column 'scaled_features'\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='scaled_features')\n",
    "# Optional: Scale the features (depending on whether you think it helps)\n",
    "#scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Severity: integer (nullable = true)\n",
      " |-- Temperature: decimal(10,0) (nullable = true)\n",
      " |-- Humidity: decimal(10,0) (nullable = true)\n",
      " |-- Pressure: decimal(10,0) (nullable = true)\n",
      " |-- Visibility: decimal(10,0) (nullable = true)\n",
      " |-- Wind_Speed: decimal(10,0) (nullable = true)\n",
      " |-- Precipitation: decimal(10,0) (nullable = true)\n",
      " |-- Weekday: integer (nullable = true)\n",
      " |-- Rush_Hour: integer (nullable = true)\n",
      " |-- Rain: integer (nullable = true)\n",
      " |-- Snow: integer (nullable = true)\n",
      " |-- SeasonVec: vector (nullable = true)\n",
      " |-- Astronomical_TwilightIndex: integer (nullable = true)\n",
      " |-- Interstate_Indicator: integer (nullable = true)\n",
      " |-- Sex_ratio: float (nullable = true)\n",
      " |-- Percent_Age_15-19: float (nullable = true)\n",
      " |-- Percent_Age_20-24: float (nullable = true)\n",
      " |-- Percent_Age_65_over: float (nullable = true)\n",
      " |-- MedianIncome: float (nullable = true)\n",
      " |-- Urban_Ratio: float (nullable = true)\n",
      " |-- Traffic_Interference: integer (nullable = true)\n",
      " |-- Traffic_Intersection: integer (nullable = true)\n",
      " |-- Destination: integer (nullable = true)\n",
      " |-- Severity_Binary: integer (nullable = false)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(labelCol=\"Severity_Binary\", featuresCol=\"scaled_features\", weightCol=\"weight\", maxIter=10)\n",
    "\n",
    "# Step 4: Create a pipeline with the VectorAssembler and GBTClassifier\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Step 5: Handle missing values - Remove rows with missing values in the required columns\n",
    "df_with_weights = df_with_weights.dropna(subset=['Severity_Binary', 'weight'])\n",
    "df_with_weights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Severity_Binary|  count|\n",
      "+---------------+-------+\n",
      "|              1| 781694|\n",
      "|              0|3435128|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Severity_Binary|  count|\n",
      "+---------------+-------+\n",
      "|              1| 260903|\n",
      "|              0|1144588|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:================================================>       (20 + 3) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Severity_Binary|  count|\n",
      "+---------------+-------+\n",
      "|              1| 260023|\n",
      "|              0|1144470|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, and test sets (e.g., 60% train, 20% validation, 20% test)\n",
    "train_df, val_df, test_df = df_with_weights.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "# Check the class distribution in each of the splits\n",
    "train_class_counts = train_df.groupBy(\"Severity_Binary\").count()\n",
    "val_class_counts = val_df.groupBy(\"Severity_Binary\").count()\n",
    "test_class_counts = test_df.groupBy(\"Severity_Binary\").count()\n",
    "\n",
    "# Show the counts for train, validation, and test sets\n",
    "train_class_counts.show()\n",
    "val_class_counts.show()\n",
    "test_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Counts (After Balancing):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===========================>                           (23 + 23) / 46]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49.831s][warning][gc,alloc] Executor task launch worker for task 33.0 in stage 29.0 (TID 251): Retried waiting for GCLocker too often allocating 131074 words\n",
      "24/12/02 18:22:10 ERROR Executor: Exception in task 33.0 in stage 29.0 (TID 251)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1572/0x0000000840af4840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[49.866s][warning][gc,alloc] Executor task launch worker for task 45.0 in stage 29.0 (TID 263): Retried waiting for GCLocker too often allocating 262146 words\n",
      "24/12/02 18:22:10 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 33.0 in stage 29.0 (TID 251),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1572/0x0000000840af4840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/12/02 18:22:10 ERROR TaskSetManager: Task 33 in stage 29.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_557486/3595977965.py\", line 21, in <module>\n",
      "    train_class_counts_after.show()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\", line 606, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2098, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\", line 540, in send_command\n",
      "    \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_557486/3595977965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Class Counts (After Balancing):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_class_counts_after\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2101\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;34m\"traceback\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;34m\"ename\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0;34m\"evalue\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         }\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# Note: technically this should return a bytestring 'str' rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, and test sets (e.g., 60% train, 20% validation, 20% test)\n",
    "train_df, val_df, test_df = df_with_weights.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "# Step 2: Get counts of each class in the training set\n",
    "class_0_count = train_df.filter(train_df['Severity_Binary'] == 0).count()\n",
    "class_1_count = train_df.filter(train_df['Severity_Binary'] == 1).count()\n",
    "\n",
    "# Step 3: Downsample the majority class (class 0) to match the minority class (class 1) in the train set\n",
    "class_0_sampled = train_df.filter(train_df['Severity_Binary'] == 0) \\\n",
    "    .sample(False, class_1_count / class_0_count, seed=42)\n",
    "\n",
    "# For class 1 (minority class), no need to sample, keep all instances\n",
    "class_1_df = train_df.filter(train_df['Severity_Binary'] == 1)\n",
    "\n",
    "# Step 4: Combine the sampled data for the balanced training set\n",
    "balanced_train_df = class_0_sampled.union(class_1_df)\n",
    "\n",
    "# Check the class distribution in the balanced train set\n",
    "train_class_counts_after = balanced_train_df.groupBy(\"Severity_Binary\").count()\n",
    "print(\"Train Class Counts (After Balancing):\")\n",
    "train_class_counts_after.show()\n",
    "\n",
    "# Cache the balanced training set if needed\n",
    "balanced_train_df.cache()\n",
    "\n",
    "# Show the counts for validation, and test sets\n",
    "val_class_counts = val_df.groupBy(\"Severity_Binary\").count()\n",
    "test_class_counts = test_df.groupBy(\"Severity_Binary\").count()\n",
    "\n",
    "# Show the counts for validation, and test sets\n",
    "val_class_counts.show()\n",
    "test_class_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbt_model = pipeline.fit(balanced_train_df)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Severity_Binary\", rawPredictionCol=\"prediction\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = gbt_model.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score\n",
    "\n",
    "# Prepare the true labels and predicted scores for the validation set\n",
    "y_true_val = val_predictions.select(\"Severity_Binary\").rdd.flatMap(lambda x: x).collect()\n",
    "y_scores_val = val_predictions.select(\"probability\").rdd.flatMap(lambda x: x).collect()\n",
    "y_scores_val = [score[1] for score in y_scores_val]  # Only take the probability for Class 1\n",
    "\n",
    "# Compute Precision-Recall curve and AUC for Validation Set\n",
    "precision_val, recall_val, _ = precision_recall_curve(y_true_val, y_scores_val)\n",
    "pr_auc_val = auc(recall_val, precision_val)\n",
    "\n",
    "# Compute Precision, Recall, F1 for Validation Set\n",
    "predictions_val_bin = [1 if score > 0.5 else 0 for score in y_scores_val]  # Apply threshold of 0.5\n",
    "precision_val_final = precision_score(y_true_val, predictions_val_bin)\n",
    "recall_val_final = recall_score(y_true_val, predictions_val_bin)\n",
    "f1_val_final = f1_score(y_true_val, predictions_val_bin)\n",
    "val_accuracy = evaluator.evaluate(val_predictions)\n",
    "\n",
    "# Print Metrics for Validation Set\n",
    "print(f\"Validation Set Metrics:\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Precision: {precision_val_final:.2f}\")\n",
    "print(f\"Recall: {recall_val_final:.2f}\")\n",
    "print(f\"F1 Score: {f1_val_final:.2f}\")\n",
    "print(f\"AUC: {pr_auc_val:.2f}\")\n",
    "\n",
    "# Plot the Precision-Recall curve for Validation Set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_val, precision_val, marker='.', label=f'AUC = {pr_auc_val:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Validation Set)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Define the paramGrid for hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [4, 6])  # Expanded depth options for more granularity\n",
    "             .addGrid(gbt.maxIter, [10, 15])   # Slightly different iteration ranges\n",
    "             #.addGrid(gbt.stepSize, [0.05, 0.2])  # Step size options\n",
    "             .build())\n",
    "\n",
    "# Step 3: Set up the cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline, \n",
    "                          estimatorParamMaps=paramGrid, \n",
    "                          evaluator=evaluator,\n",
    "                          parallelism=2, \n",
    "                          numFolds=4)  # Number of folds for cross-validation\n",
    "\n",
    "# Step 4: Train the model with cross-validation\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Step 5: Make predictions with the best model on the validation set\n",
    "cv_predictions = cv_model.transform(val_df)\n",
    "\n",
    "# Step 6: Evaluate the cross-validated model on the validation set\n",
    "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
    "print(f\"Cross-validated Accuracy on Validation Set: {cv_accuracy}\")\n",
    "\n",
    "# Step 7: Compute Precision, Recall, F1, AUC for the validation set\n",
    "y_true_val = cv_predictions.select(\"Severity_Binary\").rdd.flatMap(lambda x: x).collect()\n",
    "y_scores_val = cv_predictions.select(\"probability\").rdd.flatMap(lambda x: x).collect()\n",
    "y_scores_val = [score[1] for score in y_scores_val]  # Only take the probability for Class 1\n",
    "\n",
    "# Precision Recall Curve\n",
    "\n",
    "precision_val, recall_val, _ = precision_recall_curve(y_true_val, y_scores_val)\n",
    "pr_auc_val = auc(recall_val, precision_val)\n",
    "\n",
    "# Compute Precision, Recall, F1 for Validation Set\n",
    "predictions_val_bin = [1 if score > 0.5 else 0 for score in y_scores_val]  # Apply threshold of 0.5\n",
    "precision_val_final = precision_score(y_true_val, predictions_val_bin)\n",
    "recall_val_final = recall_score(y_true_val, predictions_val_bin)\n",
    "f1_val_final = f1_score(y_true_val, predictions_val_bin)\n",
    "\n",
    "# Print Metrics for Validation Set\n",
    "print(f\"Validation Set Metrics:\")\n",
    "print(f\"Precision: {precision_val_final:.2f}\")\n",
    "print(f\"Recall: {recall_val_final:.2f}\")\n",
    "print(f\"F1 Score: {f1_val_final:.2f}\")\n",
    "print(f\"AUC: {pr_auc_val:.2f}\")\n",
    "\n",
    "# Step 8: Get and save the best model from cross-validation\n",
    "bestModel = cv_model.bestModel\n",
    "bestModel.write().overwrite().save(\"best_gb_model_v2\")\n",
    "print(\"Best model parameters:\")\n",
    "print(bestModel.stages[-1]._java_obj.paramMap())  # Print the best model's parameters\n",
    "\n",
    "# Optionally: Display the best hyperparameters from cross-validation\n",
    "best_params = bestModel.stages[-1]._java_obj.paramMap()\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the saved best model\n",
    "#loadedGBModel = PipelineModel.load(\"best_gb_model_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importances = cv_model.bestModel.stages[-1].featureImportances\n",
    "\n",
    "# Create a mapping between feature names and their importance scores\n",
    "feature_importance_dict = {}\n",
    "feature_names = assembler.getInputCols()\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    feature_importance_dict[feature_name] = feature_importances[i]\n",
    "\n",
    "# Sort the feature importance dictionary by score in descending order\n",
    "sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importances\n",
    "for feature_name, importance_score in sorted_feature_importances:\n",
    "    print(f\"{feature_name}: {importance_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Evaluate Metrics for Test Set\n",
    "test_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Prepare the true labels and predicted scores for the test set\n",
    "y_true_test = test_predictions.select(\"Severity_Binary\").rdd.flatMap(lambda x: x).collect()\n",
    "y_scores_test = test_predictions.select(\"probability\").rdd.flatMap(lambda x: x).collect()\n",
    "y_scores_test = [score[1] for score in y_scores_test]  # Only take the probability for Class 1\n",
    "\n",
    "# Compute Precision-Recall curve and AUC for Test Set\n",
    "precision_test, recall_test, _ = precision_recall_curve(y_true_test, y_scores_test)\n",
    "pr_auc_test = auc(recall_test, precision_test)\n",
    "\n",
    "# Compute Precision, Recall, F1 for Test Set\n",
    "precision_test_final = precision_score(y_true_test, [1 if score > 0.5 else 0 for score in y_scores_test])\n",
    "recall_test_final = recall_score(y_true_test, [1 if score > 0.5 else 0 for score in y_scores_test])\n",
    "f1_test_final = f1_score(y_true_test, [1 if score > 0.5 else 0 for score in y_scores_test])\n",
    "\n",
    "# Print Metrics for Test Set\n",
    "print(f\"Test Set Metrics:\")\n",
    "print(f\"Precision: {precision_test_final:.2f}\")\n",
    "print(f\"Recall: {recall_test_final:.2f}\")\n",
    "print(f\"F1 Score: {f1_test_final:.2f}\")\n",
    "print(f\"AUC: {pr_auc_test:.2f}\")\n",
    "\n",
    "# Plot the Precision-Recall curve for Test Set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_test, precision_test, marker='.', label=f'AUC = {pr_auc_test:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Test Set)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/py4j/clientserver.py\", line 540, in send_command\n",
      "    \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert predictions to RDD for evaluation\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"Severity_Binary\")\n",
    "rdd = predictionAndLabels.rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "\n",
    "# Step 2: Compute confusion matrix using BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(rdd)  # Binary Classification Metrics\n",
    "\n",
    "# Step 3: Confusion Matrix (PySpark)\n",
    "conf_matrix = metrics.confusionMatrix().toArray()  # Confusion Matrix\n",
    "print(f\"Confusion Matrix (PySpark):\\n{conf_matrix}\")\n",
    "\n",
    "# Step 4: Convert confusion matrix to pandas DataFrame for better visualization\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1'])\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
