{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f6a102-4e48-4b09-bb89-7ac701ad50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "#import holidays\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.ml.classification import RandomForestClassifier, BinaryLogisticRegressionSummary\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6477a6-d56a-44ff-9304-860e60040873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/15 16:55:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"US_Accidents\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\") #supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491c407c-978d-4657-b228-d5b1cd183109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "df = spark.read.parquet(\"final_dataset_revised.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559036cc-f7ce-44d1-afab-f7102d37f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use StringIndexer for encoding the 'Severity' column\n",
    "indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9becd375-b12f-434a-94ab-0a270c10f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['Temperature', 'Astronomical_TwilightIndex', 'Interstate_Indicator', 'Rush_Hour', 'SeasonVec', 'Urban_Ratio', 'Weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6205116d-7ba9-462e-8bc8-908487196013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = df.randomSplit([0.8, 0.2], 314)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1edaec72-5525-4f4b-a6bd-b860a3d8633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|Severity|Temperature|Humidity|Pressure|Visibility|Wind_Speed|Precipitation|Weekday|Rush_Hour|Rain|Snow|    SeasonVec|Astronomical_TwilightIndex|Interstate_Indicator|Sex_ratio|Percent_Age_15-19|Percent_Age_20-24|Percent_Age_65_over|MedianIncome|Urban_Ratio|Traffic_Interference|Traffic_Intersection|Destination|SeverityIndex|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "|       2|          1|      66|      29|        10|        20|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     92.9|              5.0|              6.4|                7.6|    117349.0|        0.0|                   0|                   0|          0|          0.0|\n",
      "|       2|          4|      64|      30|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     99.8|              7.8|              5.9|               12.5|    141285.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|          4|      73|      30|         8|        13|            0|      1|        0|   0|   1|(3,[0],[1.0])|                         1|                   0|    102.8|             18.7|             17.7|               13.0|     51800.0|     0.9804|                   0|                   0|          0|          0.0|\n",
      "|       2|          5|      78|      29|        10|        13|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     91.5|              7.1|              6.7|               17.1|     74903.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|          7|      84|      30|        10|         0|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     93.7|              5.3|              4.9|               17.0|     96522.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|          9|      59|      30|        10|        15|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|    102.9|              7.2|              6.8|               11.0|     85185.0|     0.9943|                   0|                   0|          0|          0.0|\n",
      "|       2|         10|      64|      30|         9|        14|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     86.3|              6.2|              6.4|               19.1|     39657.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         10|      80|      29|         1|         7|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   1|    102.1|             19.0|             15.5|                7.3|     21645.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         10|      92|      29|        10|         7|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     96.0|              6.1|              6.1|               14.8|     64329.0|     0.8822|                   0|                   0|          0|          0.0|\n",
      "|       2|         11|      63|      29|        10|        18|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   1|     96.5|              7.3|              6.5|               13.0|     63184.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "|       2|         11|      70|      29|         6|        18|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     97.3|              6.0|              5.8|               17.8|     75625.0|     0.9539|                   0|                   0|          0|          0.0|\n",
      "|       2|         11|      73|      30|        10|         0|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     96.4|              7.4|              6.0|               14.8|     67525.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|         11|      82|      29|         1|         5|            0|      1|        1|   0|   1|(3,[0],[1.0])|                         0|                   0|    105.4|              6.1|              5.8|               17.5|     76852.0|     0.7284|                   0|                   0|          0|          0.0|\n",
      "|       2|         12|      62|      30|        10|         9|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     94.6|              8.0|              4.8|               18.8|     80022.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|         13|      54|      30|        10|         0|            0|      1|        0|   0|   0|(3,[0],[1.0])|                         2|                   0|     96.7|              6.2|              5.4|               16.2|     83561.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|         13|      67|      30|        10|         5|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     91.5|              5.8|              6.6|               13.8|     43330.0|     0.9131|                   0|                   0|          0|          0.0|\n",
      "|       2|         13|      81|      28|         7|         0|            0|      1|        0|   0|   1|(3,[0],[1.0])|                         1|                   0|    101.4|              5.7|              8.9|               11.8|     41401.0|        1.0|                   0|                   1|          1|          0.0|\n",
      "|       2|         14|      67|      29|        10|        12|            0|      1|        1|   0|   0|(3,[0],[1.0])|                         0|                   0|     83.9|              7.6|              5.5|               13.4|     36537.0|        1.0|                   0|                   1|          0|          0.0|\n",
      "|       2|         14|      88|      30|         1|        12|            0|      1|        0|   0|   1|(3,[0],[1.0])|                         1|                   0|     93.4|              5.0|              3.9|               21.7|     88255.0|     0.9429|                   0|                   0|          0|          0.0|\n",
      "|       2|         16|      45|      30|        10|         7|            0|      0|        0|   0|   0|(3,[0],[1.0])|                         0|                   0|     99.0|              7.9|              5.2|               18.3|    122494.0|        1.0|                   0|                   0|          0|          0.0|\n",
      "+--------+-----------+--------+--------+----------+----------+-------------+-------+---------+----+----+-------------+--------------------------+--------------------+---------+-----------------+-----------------+-------------------+------------+-----------+--------------------+--------------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (86 + 6) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|SeverityIndex|count|\n",
      "+-------------+-----+\n",
      "|          0.0|52186|\n",
      "|          1.0|52173|\n",
      "|          3.0|52136|\n",
      "|          2.0|51867|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Undersampling\n",
    "#df.cache()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'Severity' and count occurrences\n",
    "class_counts = train.groupBy(\"SeverityIndex\").count()\n",
    "\n",
    "# Step 2: Use PySpark's min() function to find the minimum count\n",
    "min_class_size = class_counts.agg(F.min('count')).collect()[0][0]\n",
    "\n",
    "undersampled_train_list = []\n",
    "\n",
    "for row in class_counts.collect():\n",
    "    class_label = row['SeverityIndex']\n",
    "    class_size = row['count']\n",
    "\n",
    "    if class_size > min_class_size:\n",
    "        # Sample the data for this class to the size of the minimum class\n",
    "        class_data = train.filter(F.col(\"SeverityIndex\") == class_label)\n",
    "        class_data_undersampled = class_data.sample(withReplacement=False, fraction=min_class_size / class_size)\n",
    "    else:\n",
    "        # For classes that are already at the minimum size, keep all samples\n",
    "        class_data_undersampled = train.filter(F.col(\"SeverityIndex\") == class_label)\n",
    "\n",
    "    undersampled_train_list.append(class_data_undersampled)\n",
    "\n",
    "# Combine all the undersampled DataFrames\n",
    "undersampled_train = undersampled_train_list[0]  # start with the first one\n",
    "for df in undersampled_train_list[1:]:\n",
    "    undersampled_train = undersampled_train.union(df)\n",
    "\n",
    "# Show the result\n",
    "undersampled_train.show()\n",
    "\n",
    "# Step 4: Group by 'Severity' and count the occurrences in the undersampled DataFrame\n",
    "undersampled_class_counts = undersampled_train.groupBy(\"SeverityIndex\").count()\n",
    "\n",
    "# Show the result\n",
    "undersampled_class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bae2ca-ae3f-4a4f-80bc-eb1ffa48c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble data for logistic regression model\n",
    "assembler = VectorAssembler(inputCols=feature_list,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "undersampled_train = assembler.transform(undersampled_train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e409958-0785-437a-9fcf-531e1d76dad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(undersampled_train)\n",
    "scaledTrainData = scalerModel.transform(undersampled_train)\n",
    "scaledTestData = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99f8f0e-20be-4a6a-b888-61e9d50e7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: maxIter=55, regParam=0.01, elasticNetParam=0.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"10-Fold Cross Validation Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the model and set the label column\n",
    "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"SeverityIndex\", family=\"multinomial\")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [10, 55, 100]) \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"SeverityIndex\", metricName=\"accuracy\")\n",
    "\n",
    "# Define the CrossValidator\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=10,  # 10-fold cross-validation\n",
    "    parallelism=4  # Number of threads for parallelism\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_model = cross_validator.fit(scaledTrainData)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_model = cv_model.bestModel\n",
    "print(f\"Best Parameters: maxIter={best_model._java_obj.getMaxIter()}, \"\n",
    "      f\"regParam={best_model._java_obj.getRegParam()}, \"\n",
    "      f\"elasticNetParam={best_model._java_obj.getElasticNetParam()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7555c62-3fb4-4aec-b885-db99e8164e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected number of classes: 4\n",
      "Coefficients: DenseMatrix([[ 0.00000000e+00,  0.00000000e+00, -6.14201020e-02,\n",
      "              -5.75887624e-04,  1.97726472e-01,  1.12447677e-01,\n",
      "               0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      "             [ 0.00000000e+00, -8.35392913e-02,  5.07788351e-01,\n",
      "               0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "              -2.17196027e-02,  2.01056121e-01,  0.00000000e+00],\n",
      "             [-1.19434540e-01,  1.65462848e-01,  0.00000000e+00,\n",
      "              -1.07126879e-01,  0.00000000e+00,  0.00000000e+00,\n",
      "               0.00000000e+00, -4.90584903e-01, -7.83493790e-02],\n",
      "             [ 2.90638696e-01,  0.00000000e+00, -3.31406806e-01,\n",
      "               2.39608233e-01, -9.11074746e-01, -7.07224898e-01,\n",
      "               1.22089459e-01,  4.56547231e-02,  2.41243174e-01]])\n",
      "Intercept: [0.048177869160951056,-0.7251656216334146,1.9968812232613184,-1.3198934707888546]\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model with intercept\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='SeverityIndex',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=55, \n",
    "                        regParam=0.01, \n",
    "                        elasticNetParam=0.5,\n",
    "                        family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledTrainData)\n",
    "print(f\"Detected number of classes: {lrModel.numClasses}\")\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2d903a-3e64-4562-b6ca-90e323b49e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       3.0|\n",
      "|       2.0|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8787:===================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3897393177472227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledTestData)\n",
    "lrPred.select(\"prediction\").distinct().show()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='SeverityIndex', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(lrPred, {evaluator.metricName: \"accuracy\"})\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d621af5-8be9-4070-95f4-e89e46f8aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8793:=================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7655735138966925\n",
      "Recall: 0.3897393177472227\n",
      "F1 Score: 0.485264392033878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "precision = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedPrecision'})\n",
    "recall = evaluator.evaluate(lrPred, {evaluator.metricName: 'weightedRecall'})\n",
    "f1_score = evaluator.evaluate(lrPred, {evaluator.metricName: 'f1'})\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3bb4567-0bfa-45f2-b38c-85b40f32572d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8795:=================================================>    (21 + 2) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|SeverityIndex|           accuracy|\n",
      "+-------------+-------------------+\n",
      "|          0.0|0.36939323078228314|\n",
      "|          1.0| 0.4679891234435923|\n",
      "|          3.0|  0.793095494387206|\n",
      "|          2.0|0.39353809110507604|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a column to indicate correct or incorrect predictions\n",
    "predictions = lrPred.withColumn(\n",
    "    'is_correct', F.expr(\"CASE WHEN SeverityIndex = prediction THEN 1 ELSE 0 END\")\n",
    ")\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracy_by_class = predictions.groupBy('SeverityIndex').agg(\n",
    "    (F.sum('is_correct') / F.count('SeverityIndex')).alias('accuracy')\n",
    ")\n",
    "\n",
    "# Show per-class accuracy\n",
    "accuracy_by_class.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
